# -*- coding: utf-8 -*-
"""DDR / ML Airline  Project .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1suOEPsdm7XH54iVzeJM3ZkyV3fV9vTyf

### ML EDA on Kaggle Dataset : https://www.kaggle.com/datasets/dilwong/flightprices/code

#Importing Relevant Libraries
"""

import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np 
import plotly
import plotly.graph_objects as go
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score
from datetime import datetime
from google.colab import drive
import re
import statsmodels.api as sm
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score
import graphviz
from sklearn.tree import export_graphviz
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
import xgboost as xgb

"""#connecting to google drive to read excel {comment out when submitting}"""

from google.colab import drive
drive.mount('/content/drive')
os.chdir('/content/drive/MyDrive')
import pandas as pd

from google.colab import files
uploaded = files.upload()

df = pd.read_excel('Minimal_Flights.xlsx')

#In this analysis, we will only be looking at performing EDA and predicting prices for non-stop flights for simplicity and ease
#Flights with multiple stops have added variables such as layover times, multiple departure an arrival times, multiple airlines etc and would
#require separate analysis
#we will filter our dataset accordingly
df = df[df['isNonStop'] == True]

df

"""#Data Cleaning"""

df.dtypes

#In "segmentsArrivalTimeRaw" column String containing the departure time (ISO 8601 format: YYYY-MM-DDThh:mm:ss.000Â±[hh]:00) 
#We will create new columns that displays arrival time to destination
# convert the datetime column to a pandas datetime object
df['segmentsArrivalTimeRaw'] = pd.to_datetime(df['segmentsArrivalTimeRaw'])

# extract the time from the datetime column and format it as a string
df['arrival_to_destination'] = df['segmentsArrivalTimeRaw'].dt.strftime('%H:%M:%S')

#Similarly, We will create new columns that displays departure time from origin
# extract the time part of the string


# extract the time component from the timestamp column
times = []
for time_str in df['segmentsDepartureTimeRaw']:
    datetime_obj = datetime.strptime(time_str, '%Y-%m-%dT%H:%M:%S.%f%z')
    time = datetime_obj.strftime('%H:%M:%S')
    times.append(time)

# create a new column with the extracted time values
df['Departure_Time_from_Origin'] = times

#next cleaning duration column
import datetime

def parse_duration(duration_str):
    duration = duration_str.replace('PT', '')
    if 'H' in duration and 'M' in duration:
        hours, minutes = duration.split('H')
        minutes = minutes.replace('M', '')
    elif 'H' in duration:
        hours, minutes = duration.split('H')
        minutes = '0'
    elif 'M' in duration:
        hours = '0'
        minutes = duration.replace('M', '')
    else:
        return datetime.timedelta()
    return datetime.timedelta(hours=int(hours), minutes=int(minutes))

# apply the parse_duration function to the travelDuration column
df['travelDuration'] = df['travelDuration'].apply(parse_duration)

# Extract hours and minutes as separate columns
df['hours'] = df['travelDuration'].astype(str).apply(lambda x: re.findall('\d+:\d+', x)[0].split(':')[0])
df['minutes'] = df['travelDuration'].astype(str).apply(lambda x: re.findall('\d+:\d+', x)[0].split(':')[1])

# Convert hours and minutes to integers
df['hours'] = df['hours'].astype(int)
df['minutes'] = df['minutes'].astype(int)

# Calculate total duration in minutes
df['total_duration_minutes'] = df['hours'] * 60 + df['minutes']

df.columns

#Getting rid of unnecessary or repetitive columns:

#legid- unique id 
#travelDuration, 'hours' and 'minutes- captured under total_duration_minutes

#isNonStop- true for all
#baseFare- we will only be considering totalfare
#segmentsDepartureTimeEpochSeconds, segmentsDepartureTimeRaw- captured under Departure_Time_from_Origin
#segmentsArrivalTimeEpochSeconds, segmentsArrivalTimeRaw- captured under arrival_to_destination
#segmentsArrivalAirportCode, destinationAirport- SFO for all
#segmentsDepartureAirportCode- captured under startingAirport
#segmentsAirlineName- captured under Airline code
#segmentsDurationInSeconds - captured under total_duration_minutes
#segmentsDistance- captured under totalTravelDistance

df.drop([ 'legId', 'travelDuration', 'hours', 'minutes', 'isNonStop', 'baseFare', 'segmentsDepartureTimeEpochSeconds', 'segmentsDepartureTimeRaw',
       'segmentsArrivalTimeEpochSeconds', 'segmentsArrivalTimeRaw', 'segmentsArrivalAirportCode', 'segmentsDepartureAirportCode',
       "segmentsAirlineName", 'segmentsDurationInSeconds', 'segmentsDistance'

         ], axis=1, inplace=True)

df

"""# Explorartory Data Analysis """

# check the shape of the DataFrame
print("DataFrame shape:", df.shape)

# check the first 5 rows of the DataFrame
print(df.head())

# check for missing values
print("Missing values:")
print(df.isna().sum())

# check data types of each column
print("Data types:")
print(df.dtypes)

#check unique flights 
df['segmentsAirlineCode'].unique

airline_counts = df['segmentsAirlineCode'].value_counts().sort_values(ascending=True)
sns.set_style("whitegrid")
colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD']

# Create horizontal bar chart of airline counts
airline_counts.plot(kind='barh', color=colors)
plt.title("Customer Count by Airline")
plt.xlabel("Count")
plt.ylabel("Airline")
plt.show()

# check the summary statistics of numerical columns
print("Summary statistics:")
print(df.describe())

# Visualizations 

plt.scatter( df['totalTravelDistance'], df['totalFare'])
print("Total Travel Distance and Total Fare")

# Plot a histogram of the base fare
plt.hist(df['totalFare'], bins=20)
plt.xlabel('Tase Fare')
plt.ylabel('Count')
plt.title('Distribution of Base Fare')
plt.show()



# Plot a scatter plot of the base fare vs total travel distance using plotly
fig = px.scatter(df, x='totalFare', y='totalTravelDistance', hover_data=['startingAirport', 'destinationAirport'])
fig.show()

# Create a bar plot of the mean total fare by airline code
sns.barplot(x='segmentsAirlineCode', y='totalFare', data=df, ci=None)
plt.xlabel('Airline Code')
plt.ylabel('Mean Total Fare')
plt.title('Mean Total Fare by Airline Code')
plt.show()

#Price and Airline
sns.catplot(y = "totalFare", x = "segmentsAirlineCode", data = df.sort_values("totalFare", ascending = False), kind="boxen", height = 8, aspect = 3)
plt.show()

# Create a violin plot of the total fare by cabin code
sns.violinplot(x='segmentsCabinCode', y='totalFare', data=df)
plt.xlabel('Cabin Code')
plt.ylabel('Total Fare')
plt.title('Total Fare by Cabin Code')
plt.show()


# Create a line plot of the total fare over time
df['searchDate'] = pd.to_datetime(df['searchDate'])
df = df.sort_values(by='searchDate')
plt.plot(df['searchDate'], df['totalFare'])
plt.xlabel('Search Date')
plt.ylabel('Total Fare')
plt.title('Total Fare Over Time')
plt.show()

# checking distribution of the duration 
sns.distplot(df['total_duration_minutes'],hist = False,color = "green")
plt.show()

#  We can see that we have outliers in duration feature but we will not replace them
#  in place of replacing them we will reduce their effect as it is 
#  the real time, which is taken by flight from source and destination
df.boxplot(column ='total_duration_minutes')

# Create a scatter plot of the total fare vs travel duration using plotly
fig = px.scatter(df, x='total_duration_minutes', y='totalFare', hover_data=['startingAirport', 'destinationAirport'])
fig.show()

#Pearson Correlation 
plt.figure(figsize=(25,10))
cor = df.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()

#Correlation with target variable price
cor_target = abs(cor["totalFare"])

relevant_features = cor_target
relevant_features

"""# Does ticket price change based on the departure time and arrival time?

We will run a linear regression to see if the departure time and arrival time variables are significant and how they affect the total price for each flight on average
"""

df_copy = df.copy() #we will make a copy of the original df so as to have the original df unmodified futher

df_copy

#getting rid of tow more columns- destination airport as it will be SFO for all, elapsed days as it is mostly 0
df_copy = df_copy.drop(['destinationAirport', 'elapsedDays'], axis=1)

#visualizing distribution of total fares
plt.hist(df_copy['totalFare'], bins=20, color='blue')
plt.xlabel('Total Fare')
plt.ylabel('Frequency')
plt.title('Histogram of Total Fares')
plt.show()

#As seen from the plot there is a slight positive skew to our graph

# Convert categorical variables into dummy variables
df_copy = pd.get_dummies(df_copy, columns=['fareBasisCode','startingAirport', 'segmentsAirlineCode', 'segmentsEquipmentDescription', 'segmentsCabinCode'])
# Convert boolean variables into binary variables
df_copy['isBasicEconomy'] = df_copy['isBasicEconomy'].astype(int)
df_copy['isRefundable'] = df_copy['isRefundable'].astype(int)

# Extract year, month, and day from flightDate
df_copy['flightYear'] = df_copy['flightDate'].dt.year
df_copy['flightMonth'] = df_copy['flightDate'].dt.month
df_copy['flightDay'] = df_copy['flightDate'].dt.day

#Transform the data: Convert Departure_Time_from_Origin and arrival_to_destination columns to datetime format, and extract the hour and minute 
#components of the time.
df_copy['Departure_Time_from_Origin'] = pd.to_datetime(df_copy['Departure_Time_from_Origin'])
df_copy['arrival_to_destination'] = pd.to_datetime(df_copy['arrival_to_destination'])
df_copy['Departure_Hour'] = df_copy['Departure_Time_from_Origin'].dt.hour
df_copy['Departure_Minute'] = df_copy['Departure_Time_from_Origin'].dt.minute
df_copy['Arrival_Hour'] = df_copy['arrival_to_destination'].dt.hour
df_copy['Arrival_Minute'] = df_copy['arrival_to_destination'].dt.minute

# Extract day of the week from flightDate
df_copy['DayOfWeek'] = df_copy['flightDate'].dt.dayofweek

# Drop flightDate column and search date column
df_copy.drop(['flightDate', 'searchDate'], axis=1, inplace=True)

#Visualize the data: Plot the ticket price against the departure and arrival times to identify any patterns or trends.
# plot ticket price against departure hour
sns.lineplot(x='Departure_Hour', y='totalFare', data=df_copy)
plt.show()

# plot ticket price against arrival hour
sns.lineplot(x='Arrival_Hour', y='totalFare', data=df_copy)
plt.show()

#in our first linear regression model, we will only select the departrue and arrival time variables
# select features and target variable
X = df_copy[['Departure_Hour', 'Departure_Minute', 'Arrival_Hour', 'Arrival_Minute']]
y = df_copy['totalFare']

# split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# train the model
model = LinearRegression()
model.fit(X_train, y_train)

# make predictions on the test set
y_pred = model.predict(X_test)

# evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print('MSE:', mse)
print('R2 score:', r2)

# Calculate p-values using statsmodels
X_train = sm.add_constant(X_train) # add a constant term for the intercept
model_sm = sm.OLS(y_train, X_train).fit()
p_values = model_sm.summary2().tables[1]['P>|t|']

# Print p-values
print(p_values)

# get the coefficients of the model
coefficients = pd.DataFrame({'features': X.columns, 'coefficients': model.coef_})
print(coefficients)

#the feature 'Departure_Hour' has a negative coefficient of  -4.280101, which means that as the departure hour increases, 
#the ticket price tends to decrease. 

#On the other hand, 'Arrival_Hour' has a positive coefficient of  6.417603, indicating that as the arrival hour increases, the 
#ticket price tends to increase. The feature 'Arrival_Minute' also has a positive coefficient of 0.345, suggesting that as the arrival minute increases, the ticket price tends to increase slightly.

#Overall, the coefficients suggest that departure time and arrival time have a moderate influence on the ticket price. However, 
#it's important to note that the R2 score is low, indicating that the model does not explain much of the variance in the ticket prices. Therefore, further analysis or feature engineering may be necessary to improve the model'

#Next, we will first create another  regression with all covariates to get a even better estimate of the impact of
#departure and arrival time to the total fare. Since we are using all covariates, we will use LASSO for important feature selection and to avoid overfitting

X_2 = df_copy.drop(['Departure_Time_from_Origin', 'arrival_to_destination', 'totalFare'], axis=1)
Y_2 = df_copy['totalFare']

from sklearn.linear_model import Lasso

# split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_2, Y_2, test_size=0.2, random_state=42)

# train the model
model = Lasso(alpha=0.1)
model.fit(X_train, y_train)

# make predictions on the test set
y_pred = model.predict(X_test)

# evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print('MSE:', mse)
print('R2 score:', r2)

#The MSE and R2 have improved a lot using the lasso regression on all covariates vs using least squares linear regression on only the 
#arrival and departure times

# get the coefficients 
# get the coefficients
coef = model.coef_

# create a dictionary to map feature names to coefficients
coef_dict = dict(zip(X.columns, coef))

# print the coefficients for selected features
print("Departure_Hour coefficient:", coef_dict['Departure_Hour'])
print("Departure_Minute coefficient:", coef_dict['Departure_Minute'])
print("Arrival_Hour coefficient:", coef_dict['Arrival_Hour'])
print("Arrival_Minute coefficient:", coef_dict['Arrival_Minute'])

#The coefficient for 'Departure_Hour' is --130.81.89, which means that for each increase of 1 hour in the departure time, 
#the predicted flight price will decrease by $130.8, holding all other variables constant. This is a huge change compared to the 
#first model using only 4 covariates

#The coefficient for 'Arrival_Hour' is 1.29, which means that for each increase of 1 hour in the arrival time, 
#the predicted flight price will increase by $1.29, holding all other variables constant.

"""# How prices changes as number of days before flight date changes"""

df_2 = df.copy() #we will make another copy of the original df

# Convert flightDate to datetime format
df_2['flightDate'] = pd.to_datetime(df_2['flightDate'], format='%Y-%m-%d')

# Calculate number of days before flight date
df_2['days_before_flight'] = (df_2['flightDate'] - df_2['searchDate']).dt.days

# Create linear regression model
X = df_2[['days_before_flight']]
y = df_2['totalFare']
reg = LinearRegression().fit(X, y)

# Print the coefficients of the linear regression model
print('Coefficients:', reg.coef_)

#In this code, we first convert the flightDate column to datetime format, and then calculate the number of days before the flight 
#date for each row using the current date as a reference point.
#We then create a linear regression model with Days_Before_Flight as the feature and totalFare as the target variable.
#The fit method is used to train the linear regression model on the training data.
#Finally, we print the coefficients of the linear regression model to see the relationship between the number of days before 
#the flight date and ticket prices. This will tell us how much ticket prices change as we move closer to the flight date.

#The coefficient value of -0.44800165 represents the slope of the linear regression line between the feature variable (days before flight) 
#and the target variable (total fare). Specifically, it tells us how much the total fare is expected to decrease for each unit increase 
#in the number of days before the flight. In this case, a coefficient of -0.44800165 suggests that the ticket prices decrease as the 
#number of days before the flight increases.

#However, it is important to note that the coefficient only represents the relationship between the two variables and cannot establish causation. Other factors, such as demand and availability, may also affect the ticket prices.

"""# How does price change with source and destination? 


"""

#creating a new df copy to avoid making chnages to the original one 
df_6 = df.copy()

#create pivot table with aiports: origin and destination with total fare 
pd.pivot_table(data = df,columns = "destinationAirport",index = "startingAirport", values = "totalFare")

# Plotting the information regarding price variance between cities
plot_fare_dest = pd.crosstab(columns = df_6["destinationAirport"],index = df_6["startingAirport"], values = df_6["totalFare"], aggfunc="mean")
plot_fare_dest.plot(kind = "bar",figsize = (15,5))
print("The price of flight from LAX to SFO is the least expensive whereas the flights from CLT to SFO and MIA to SFO are the most expensive")

# Plot a scatter plot between total travel distance and total fare
sns.scatterplot(x="totalTravelDistance", y="totalFare", data=df)
plt.xlabel("Total Travel Distance")
plt.ylabel("Total Fare")
plt.show()

# Compute the correlation matrix
corr = df[["totalTravelDistance", "totalFare"]].corr()

# Plot the correlation matrix as a heatmap
sns.heatmap(corr, annot=True)
plt.show()
print("The correlation plot with a correlation coefficient of 0.2 between total travel distance between starting airport and destination airport indicates a weak positive correlation between the two variables being analyzed. It suggests that as total travel distance variable increases, the price also tends to increase, but the relationship between the two is not very strong.")

# Encode destination and  startingAirport as numeric values
airports = df_6['startingAirport'].unique()
airport_map = {airport: i for i, airport in enumerate(airports)}
df_6['origin_code'] = df_6['startingAirport'].map(airport_map)

destinations = df_6['destinationAirport'].unique()
destination_map = {dest: i for i, dest in enumerate(destinations)}
df_6['destination_code'] = df_6['destinationAirport'].map(destination_map)

# Split data into training and testing sets
train_data = df_6.sample(frac=0.8, random_state=42)
test_data = df_6.drop(train_data.index)

# Train linear regression model
model = LinearRegression()
features = ['origin_code', 'destination_code']
target = 'totalFare'
model.fit(train_data[features], train_data[target])

# Evaluate model on test data
predictions = model.predict(test_data[features])
error = predictions - test_data[target]
mean_absolute_error = error.abs().mean()
print(f'Mean absolute error: {mean_absolute_error:.2f}')
#Intercept
print('Intercept: ', model.intercept_)
# regression coefficients
print('Coefficients: ', model.coef_)

print("The absence of a meaningful correlation between the destination airport and the flight price is shown by the 0 coefficient for the destination airport code.However, we know there are several factors that come in to play when predicting total fare thus the main reason for regression was to understand correlation and significance of sole variables")

"""# Does ticket price change based on the day of week/ week of month ?"""

#creating a new df copy to avoid making chnages to the original one 
df_c1 = df.copy()

df_c1

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso

df_c1['weekday'] = df_c1['flightDate'].dt.dayofweek
df_c1['day'] = df_c1['flightDate'].dt.day

# Convert categorical variables into dummy variables
df_c1 = pd.get_dummies(df_c1, columns=['fareBasisCode','startingAirport', 'segmentsAirlineCode', 'segmentsEquipmentDescription', 'segmentsCabinCode'])
# Convert boolean variables into binary variables
df_c1['isBasicEconomy'] = df_c1['isBasicEconomy'].astype(int)
df_c1['isRefundable'] = df_c1['isRefundable'].astype(int)


# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

exclude_cols = ['totalFare','searchDate','flightDate', 'destinationAirport', 'elapsedDays', 'Departure_Time_from_Origin', 'arrival_to_destination'] #drop non-numeric columns
X = df_c1.drop(exclude_cols, axis=1) #input variables
y = df_c1['totalFare']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#Set penalties of variables of interest to 0
feature_names = list(X.columns)
a = []
for i in range(len(feature_names)):
  if feature_names[i] in ['weekday','day']: a.append(0)
  else: a.append(0.1)

a_array = np.array(a)

# Defining Lasso regression model
lasso_reg = Lasso(alpha=0.1, fit_intercept=True)

# Fitting the model on the training data
lasso_reg.fit(X_train, y_train)

# Count the number of nonzero coefficients
print("Number of nonzero coefficients: ",str(np.count_nonzero(lasso_reg.coef_)))

# Predicting the target variable for test data
y_pred = lasso_reg.predict(X_test)

# Evaluate the model
from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean squared error: {:.2f}".format(mse))
print("R2 score: {:.2f}".format(r2))

print(a_array)

# get feature names from the dataset
feature_names = list(X.columns) #store variable names in a list
nonzero_coef_names = [] #initialize list
nonzero_coef_values = []

print("Nonzero coefficients:")
for i in range(len(feature_names)):
  if lasso_reg.coef_[i] != 0: 
    nonzero_coef_names.append(feature_names[i]) #append nonzero coefficient names in list
    nonzero_coef_values.append(lasso_reg.coef_[i]) #append nonzero coefficient values in list
    print(feature_names[i],' ',str(round(lasso_reg.coef_[i],4)))

"""b) How is the price affected when tickets are bought just 1 or 2 days before departure?"""

df_c3 = df.copy() #we will make a copy of the original df so as to have the original df unmodified futher

# calculate the difference between the dates
df_c3['date_diff'] = (df_c3['searchDate'] - df_c3['flightDate']).abs()

# create a calculated field to check if the difference is within 2 days
df_c3['within_2_days'] = df_c3['date_diff'].apply(lambda x: 1 if x.days <= 2 else 0)

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

#exclude_cols = ['totalFare','legId','searchDate','flightDate','travelDuration','segmentsDepartureTimeEpochSeconds','segmentsDepartureTimeRaw','segmentsArrivalTimeEpochSeconds','segmentsArrivalTimeRaw','segmentsDurationInSeconds','segmentsDistance', 'weekday', 'day','date_diff'] #drop non-numeric columns
#X = df.drop(exclude_cols, axis=1) #input variables
#y = pd.DataFrame(df, columns = ['totalFare'])

# Convert categorical variables into dummy variables
df_c3 = pd.get_dummies(df_c3, columns=['fareBasisCode','startingAirport', 'segmentsAirlineCode', 'segmentsEquipmentDescription', 'segmentsCabinCode'])
# Convert boolean variables into binary variables
df_c3['isBasicEconomy'] = df_c3['isBasicEconomy'].astype(int)
df_c3['isRefundable'] = df_c3['isRefundable'].astype(int)


# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

exclude_cols = ['totalFare','searchDate','flightDate', 'destinationAirport', 'elapsedDays', 'Departure_Time_from_Origin', 'arrival_to_destination', 'date_diff'] #drop non-numeric columns
X = df_c3.drop(exclude_cols, axis=1) #input variables
y = df_c3['totalFare']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Defining Lasso regression model
lasso_reg = Lasso(alpha=0.1)

# Fitting the model on the training data
lasso_reg.fit(X_train, y_train)

# Count the number of nonzero coefficients
print("Number of nonzero coefficients: ",str(np.count_nonzero(lasso_reg.coef_)))

# Predicting the target variable for test data
y_pred = lasso_reg.predict(X_test)

# Evaluate the model
from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean squared error: {:.2f}".format(mse))
print("R2 score: {:.2f}".format(r2))

# get feature names from the dataset
feature_names = list(X.columns) #store variable names in a list
nonzero_coef_names = [] #initialize list
nonzero_coef_values = []

print("Nonzero coefficients:")
for i in range(len(feature_names)):
  if lasso_reg.coef_[i] != 0: 
    nonzero_coef_names.append(feature_names[i]) #append nonzero coefficient names in list
    nonzero_coef_values.append(lasso_reg.coef_[i]) #append nonzero coefficient values in list
    print(feature_names[i],' ',str(round(lasso_reg.coef_[i],4)))

"""# MODEL SELECTION: PRICE PREDICTION 
Using Linear regression, xgboost, decision tree and random forest
"""

#PART 1
#CART AND RF TO DETERMINE IMPORTANT FEATURES IN PREDICTING PRICE

df_ = df.copy() #making another copy of the original dataframe

df_ = df_.drop(['destinationAirport', 'elapsedDays'], axis=1)

#converting date to numerical representation
# Extract year, month, day, hour, and minute
df_['year'] = df_['flightDate'].dt.year
df_['month'] = df_['flightDate'].dt.month

#converting times to numerical representation
#Transform the data: Convert Departure_Time_from_Origin and arrival_to_destination columns to datetime format, and extract the hour and minute 
#components of the time.
df_['Departure_Time_from_Origin'] = pd.to_datetime(df_['Departure_Time_from_Origin'])
df_['arrival_to_destination'] = pd.to_datetime(df_['arrival_to_destination'])
df_['Departure_Hour'] = df_['Departure_Time_from_Origin'].dt.hour
df_['Departure_Minute'] = df_['Departure_Time_from_Origin'].dt.minute
df_['Arrival_Hour'] = df_['arrival_to_destination'].dt.hour
df_['Arrival_Minute'] = df_['arrival_to_destination'].dt.minute

df_.drop(['Departure_Time_from_Origin', 'arrival_to_destination', 'flightDate', 'searchDate'

         ], axis=1, inplace=True)

# Convert categorical variables into dummy variables
df_ = pd.get_dummies(df_, columns=['fareBasisCode','startingAirport', 'segmentsAirlineCode', 'segmentsEquipmentDescription', 'segmentsCabinCode'])
# Convert boolean variables into binary variables
df_['isBasicEconomy'] = df_['isBasicEconomy'].astype(int)
df_['isRefundable'] = df_['isRefundable'].astype(int)

#REGRESSION TREE
# Define input and target variables
X = df_.drop('totalFare', axis=1)
y = df_['totalFare']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the hyperparameters to tune
param_grid = {
    'max_depth': range(1, 11),
    'min_samples_split': range(2, 11),
    'min_samples_leaf': range(1, 11)
}

# Create a decision tree regressor
tree_reg = DecisionTreeRegressor(random_state=42)

# Use GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(tree_reg, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)

# Fit the model using the best hyperparameters and evaluate it using cross-validation
tree_reg = grid_search.best_estimator_
scores = cross_val_score(tree_reg, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
rmse_scores = np.sqrt(-scores)
print("Cross-validation scores: ", rmse_scores)
print("Mean cross-validation score: ", rmse_scores.mean())

# Calculate the MSE on the test set
y_pred = tree_reg.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("MSE on test set: ", mse)

# Calculate the mean squared error of the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X)
mse = mean_squared_error(y, y_pred)
print("Mean squared error:", mse)

#RANDOM FOREST
# create random forest regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
# fit model and perform cross validation
cv_scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')
print("Cross-validation scores: ", -cv_scores)
print("Average MSE: ", -cv_scores.mean())

# fit the model on the entire data
rf.fit(X, y)

# print feature importances
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]
print("Feature ranking:")
for f in range(X.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

#Identifying important features

# create a dictionary of feature names and their importance values
importances = dict(zip(X_train.columns, rf.feature_importances_))

# sort the dictionary by importance value in descending order
sorted_importances = sorted(importances.items(), key=lambda x: x[1], reverse=True)

# print the feature rankings
print('Feature ranking:')
for i, (feature, importance) in enumerate(sorted_importances):
    print(f"{i+1}. {feature} ({importance:.6f})")

# PART 2
#CREATING FUNCTION TO PERFORM LILinear regression, xgboost, decision tree and random forest
import xgboost as xgb
from sklearn.metrics import mean_squared_error as MSE
from sklearn.model_selection import cross_val_score
#We will use the df_ dataset from previous part

X = df_.drop('totalFare', axis=1)
y = df_['totalFare']

# splitting the data for training as well as testing purpose with 80:20 split
x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)

#Defining a function that fits all models and outputs all regression metrics for both training and testing 
def get_prediction(model):
    scores = cross_val_score(model, X, y, cv=5)
    model.fit(x_train,y_train)
    y_pred = model.predict(x_train)
    r2_tr  = r2_score(y_train,y_pred)
    y_pred1 = model.predict(x_test)
    r2_te  = r2_score(y_test,y_pred1)
    rmse = np.sqrt(MSE(y_test, y_pred1))
    return print(f"""At Training: \nR2_Score: {r2_tr} \nRMSE: {rmse} \nCVScore: {scores}
    \nAt Testing: \nR2_Score: {r2_te} \nRMSE: {rmse} \nCVScore: {scores}""")

#  WITHOUT SCALING 
# Regress using linear regression, decision tree, random forest and xgboost 
lst = [("Linear Regression",LinearRegression()),("Decision Tree",DecisionTreeRegressor()),
      ("Random Forest",RandomForestRegressor()),
      ("XG Boost",xgb.XGBRegressor())]

#print outputs 
for name,model in lst:
    print(f"The Performance of {name} without Scaling::")
    get_prediction(model)
    print("*"*50)

# WITH SCALING 
from sklearn.preprocessing import StandardScaler
std = StandardScaler()
x_train = pd.DataFrame(std.fit_transform(x_train), columns = x_train.columns)
x_test = pd.DataFrame(std.transform(x_test),columns = x_test.columns)

#  WITH SCALING 
# Regress using linear regression, decision tree, random forest and xgboost 
lst = [("Linear Regression",LinearRegression()),("Decision Tree",DecisionTreeRegressor()),
      ("Random Forest",RandomForestRegressor()),
      ("XG Boost",xgb.XGBRegressor())]

for name,model in lst:
    print(f"The Performance of {name} with Scaling::")
    get_prediction(model)
    print("*"*50)

# INTERPRETATION ON MODEL SELECTION 
'''
Based on above all four models - Linear Regression, Decision Tree, Random Forest, and XG Boost seem to perform well on the training and testing data. 
Some key differences for each : 

Linear Regression has very high R^2 of 0.98 for training, however it is not performing well at testing with a negative R^2 value. The RMSE value is the highest among all indicating low to no accuracy in predictng flight prices. Thus, Linear Regression is not the most reliable model
Decision Tree has a perfect R^2 of  0.99 out of sample R^2. The RMSE for the same is 19.98 which is compareable to few other models but we can conclude that decision tree is a better fit as compared to Linear regression
Random Forest also has a very high R^2 of 0.99 similar to decision tree. The RMSE is the lowest with about 18.96 indicating the model is doing a better job at predicting total fare on unseen data. 
XGBoost also has a lower R^2 as compared to decision tree and random forest with 0.96 with a higher RMSE when compared to Decision Tree and Random Forest indicating a its not the best choice of model.


Random Forest is the best model predictor out of those explored to predict total fare 
'''

#some additional metrics that we can calculate and plot for each model:

import xgboost as xgb
from sklearn.metrics import mean_squared_error as MSE, mean_absolute_error as MAE, explained_variance_score as EVS
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

#We will use the df_ dataset from previous part

X = df_.drop('totalFare', axis=1)
y = df_['totalFare']

# splitting the data for training as well as testing purpose with 80:20 split
x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)

#Defining a function that fits all models and outputs all regression metrics for both training and testing 
def get_prediction(model):
    scores = cross_val_score(model, X, y, cv=5)
    model.fit(x_train,y_train)
    y_pred = model.predict(x_train)
    r2_tr  = r2_score(y_train,y_pred)
    y_pred1 = model.predict(x_test)
    r2_te  = r2_score(y_test,y_pred1)
    rmse = np.sqrt(MSE(y_test, y_pred1))
    mae = MAE(y_test, y_pred1) #Mean Absolute Error (MAE): This is the average absolute difference between the predicted and actual values. It gives an idea of how far off the predictions are from the actual values.
    evs = EVS(y_test, y_pred1) #Explained Variance Score (EVS): This is the proportion of variance in the target variable that can be explained by the model. It gives an idea of how well the model is able to capture the underlying patterns in the data. 
    
    # Plot predicted vs actual values
    plt.scatter(y_test, y_pred1)
    plt.xlabel('Actual')
    plt.ylabel('Predicted')
    plt.title('Predicted vs Actual')
    plt.show()
    
    return print(f"""At Training: \nR2_Score: {r2_tr} \nRMSE: {rmse} \nCVScore: {scores}
    \nAt Testing: \nR2_Score: {r2_te} \nRMSE: {rmse} \nMAE: {mae} \nEVS: {evs} \nCVScore: {scores}""")

#  WITHOUT SCALING 
# Regress using linear regression, decision tree, random forest and xgboost 
lst = [("Linear Regression",LinearRegression()),("Decision Tree",DecisionTreeRegressor()),
      ("Random Forest",RandomForestRegressor()),
      ("XG Boost",xgb.XGBRegressor())]


#print outputs 
for name,model in lst:
    print(f"The Performance of {name} without Scaling::")
    get_prediction(model)
    print("*"*50)

#For Linear Regression, the model's R2 score is 0.98 for the training set and a negative value of -1012459.01 for the testing set. This suggests that 
#the model is overfitting, as the R2 score for the testing set is negative, which implies that the model is performing worse than a random guess. 
#Additionally, the RMSE for both training and testing sets is 222554.25, which is a high value. For Decision Tree, the model is performing well, 
#with an R2 score of 0.999 for the training set and 0.994 for the testing set. The RMSE is low for both training and testing sets, 17.78. 
#The model also has high CV scores, indicating that it performs consistently across different samples. For Random Forest, the model has an R2 score of 0.998 
#for the training set and 0.993 for the testing set. The RMSE is 18.76, indicating that the model is performing well. Similar to the Decision Tree model, 
#the Random Forest model also has high CV scores, indicating consistent performance across different samples. For XG Boost, the model's R2 score is 0.968 
#for the training set and 0.966 for the testing set. The RMSE for both training and testing sets is 40.76, which is higher than that of the Decision 
#Tree and Random Forest models. The MAE for the testing set is also high, indicating that the model's predictions are not accurate.

#On examining the performance of these models, we can see that the Decision Tree and Random Forest models outperformed the other models in terms of their 
#R2 score, which is a measure of how well the model fits the data. The Random Forest model, in particular, consistently performed well in both the 
#training and testing datasets, indicating that it has a low tendency to overfit or underfit the data.
#In addition to the R2 score, other metrics were also calculated, including root mean squared error (RMSE), mean absolute error (MAE), and explained 
#variance score (EVS). The Random Forest model had the lowest RMSE and MAE scores, indicating that it has the lowest prediction error among all the models. 
#The EVS score, which measures the proportion of variance in the dependent variable that is explained by the independent variables, was also high for the 
#Random Forest model.
#Therefore, based on the evaluation metrics, it can be concluded that the Random Forest model is the most suitable model for predicting the flight fare in this scenario. 
#It provides consistent and accurate predictions with a low prediction error, making it a reliable model for practical use.